% Solve an Input-Output Fitting problem with a Neural Network
% Script generated by Neural Fitting app
% Created Tue Feb 23 16:03:26 IST 2016
%
% This script assumes these variables are defined:
%
%   X - input data.
%   Y_n - target data.
%  X1 = 0:0.002:1;
%  Y1 = exp(tanh(2*pi*X1));    %2 for 501 samples observed 4 also for 101
%  samples
%  Y_n1 = awgn(Y1,0);
% INPUT = load('Train_101');
% X1 = INPUT.X;
% Y1 = INPUT.Y;
% Y_n1 = INPUT.Y_n;
%  figure
%  plot(X1,Y1);
%  hold on;
%  plot(X1,Y_n1,'or');


% xStruct = load('UNIVARIATE_X_501');
% tStruct = load('UNIVARIATE_Y_n_501');
% 
% x = xStruct.X;
% org = exp(tanh(2*pi*x));
% t = tStruct.Y_n;
INPUT = load('Train_501.mat');
x = INPUT.X;
t = INPUT.Y_n
org = INPUT.Y;
% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. NFTOOL falls back to this in low memory situations.
trainFcn = 'trainlm';  % Levenberg-Marquardt

% Create a Fitting Network
hiddenLayerSize = 1:10;
minError = Inf;
minINdex = -1;

trainPerformance = zeros(1,20)
valPerformance = zeros(1,20)
testPerformance = zeros(1,20)
AverageValPerformanc = zeros(1,5);

for i = 1 : 20
net = fitnet(i,trainFcn);

% Choose Input and Output Pre/Post-Processing Functions
% For a list of all processing functions type: help nnprocess
net.input.processFcns = {'removeconstantrows','mapminmax'};
net.output.processFcns = {'removeconstantrows','mapminmax'};

% %Setup Division of Data for Training, Validation, Testing
% %For a list of all data division functions type: help nndivide
% net.divideFcn = 'dividerand';  % Divide data randomly
% net.divideMode = 'sample';  % Divide up every sample
% net.divideParam.trainRatio = 65/100;
% net.divideParam.valRatio = 25/100;
% net.divideParam.testRatio = 15/100;

net.divideFcn = 'divideind';
net.divideParam.trainInd = INPUT.TrainInd;
net.divideParam.valInd = INPUT.ValIndices;
net.divideParam.testInd = INPUT.TestIndices;

% Choose a Performance Function
% For a list of all performance functions type: help nnperformance
net.performFcn = 'mse';  % Mean squared error

% Choose Plot Functions
% For a list of all plot functions type: help nnplot
net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
  'plotregression', 'plotfit'};
 
% Train the Network
[net,tr] = train(net,x,t);

% Test the Network
y = net(x);
e = gsubtract(t,y);
performance = perform(net,t,y)

% Recalculate Training, Validation and Test Performance
trainTargets = t .* tr.trainMask{1};
valTargets = t  .* tr.valMask{1};
testTargets = t  .* tr.testMask{1};
trainPerformance(i) = perform(net,trainTargets,y)
valPerformance(i) = perform(net,valTargets,y)
testPerformance(i) = perform(net,testTargets,y)

if valPerformance(i) < minError
    minError = valPerformance(i);
    minINdex = i;
    bestNet = net;
    bestTr = tr;
end

% View the Network
end


view(bestNet)



% Test the Network
y = bestNet(x);
e = gsubtract(t,y);
performance = perform(bestNet,t,y)

% Recalculate Training, Validation and Test Performance
trainTargets = t .* bestTr.trainMask{1};
xTrain = x .*bestTr.trainMask{1};
trainModel = y .* bestTr.trainMask{1};

valTargets = t  .* bestTr.valMask{1};
xVal = x .* bestTr.valMask{1};
valModel = y .* bestTr.valMask{1};

testTargets = t  .* bestTr.testMask{1};
xTest = x.*bestTr.testMask{1};
testModel = y .* bestTr.testMask{1};

trainPerform = perform(bestNet,trainTargets,y)
valPerform = perform(bestNet,valTargets,y)
testPerform = perform(bestNet,testTargets,y)
% Plots
% Uncomment these lines to enable various plots.
 figure, plotperform(bestTr)
 %to plot approx function
 
 XNew = 0 : 0.001:1;
 orgNew = bestNet(XNew);
% %figure, plottrainstate(tr)
figure, plotfit(bestNet,x,t);
figure;
plot(XNew,orgNew,'-r','LineWidth',2);

hold on;
plot(xTrain,trainTargets,'og');
plot(x,org,'-b','LineWidth',2);
legend('Approximated function','Training Points','Function');
  figure, plotfit(bestNet,xTrain,trainTargets);
  figure, plotfit(bestNet,xVal,valTargets);
 figure, plotfit(bestNet,xTest,testTargets);
% axis([0 1 0 5])
% figure, plotregression(t,y)
% figure, plotregression(trainModel,trainTargets);
% figure, plotregression(valModel,valTargets);
% figure, plotregression(testModel,testTargets);

%to plot error plots
% 
 

%this is for model Output verses taget output
h = figure
plot(xTrain,trainTargets,'or');
hold on;
plot(xTrain,trainModel,'ob');
title('Target Output VS Model Output for training Data');
plot(x,org,'-g','LineWidth',0.5);
legend('Target Output','Model Output','Function');

saveas(h, ['TOMO  train' num2str(size(x,2)) '.fig']); 

h = figure
plot(xTest,testTargets,'or');
hold on;
plot(xTest,testModel,'ob');
title('Target Output VS Model Output for testing Data');
plot(x,org,'-g','LineWidth',0.5);
legend('Target Output','Model Output','Function');

saveas(h, ['TOMO  test' num2str(size(x,2)) '.fig']); 


figure
plot(xVal,valTargets,'or');
hold on;
plot(xVal,valModel,'ob');
title('Target Output VS Model Output for Validation Data');
plot(x,org,'-g','LineWidth',0.5);
legend('Target Output','Model Output','Function');

saveas(h, ['TOMO  val' num2str(size(x,2)) '.fig']);



h = figure;
scatter(trainTargets,trainModel,'o','LineWidth',2');
title('Training Data Scatter plot');
ylabel('Model Output');
xlabel('Target Output');
hold on;
plot(0:5,0:5,'-r','LineWidth',1');
saveas(h, ['scatter  Train' num2str(size(x,2)) '.png']);


h = figure;
scatter(valTargets,valModel,'o','LineWidth',2');
title('Validation Data');
ylabel('Model Output');
xlabel('Target Output');
hold on;
plot(0:5,0:5,'-r','LineWidth',1');
saveas(h, ['scatter  Val' num2str(size(x,2)) '.png']);

h = figure;
scatter(testTargets,testModel,'o','LineWidth',2');
hold on;
ylabel('Model Output');
xlabel('Target Output');
title('Testing Data');
plot(0:5,0:5,'r','LineWidth',2');
saveas(h, ['scatter  Test' num2str(size(x,2)) '.png']);

figure;
hold on;
plot([1:20],trainPerformance,'r','LineWidth',2);
plot([1:20],valPerformance,'g','LineWidth',2);
plot([1:20],testPerformance,'b','LineWidth',2);
ylabel('MSError');
xlabel('No of Hidden layers');
legend('Train Data','Validation Data','Test Data');

%figure, ploterrhist(e)

% Deployment
% Change the (false) values to (true) to enable the following code blocks.
if (false)
  % Generate MATLAB function for neural network for application deployment
  % in MATLAB scripts or with MATLAB Compiler and Builder tools, or simply
  % to examine the calculations your trained neural network performs.
  genFunction(net,'myNeuralNetworkFunction');
  y = myNeuralNetworkFunction(x);
end
if (false)
  % Generate a matrix-only MATLAB function for neural network code
  % generation with MATLAB Coder tools.
  genFunction(net,'myNeuralNetworkFunction','MatrixOnly','yes');
  y = myNeuralNetworkFunction(x);
end
if (false)
  % Generate a Simulink diagram for simulation or deployment with.
  % Simulink Coder tools.
  gensim(net);
end

% figure
% plot(X,Y_n);
% X1 = 0:0.05:1;
% Y1 = exp(tanh(2*pi*X));
% hold on;
% plot(X,y);
% hold on;
% plot(X,Y1);